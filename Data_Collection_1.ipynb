{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "215b8fb1-6a2c-4866-a0af-b543de71aacb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping Alt News...\n",
      "Scraping Boom Live...\n",
      "Scraping Factly...\n",
      "✅ Collected 15 fake news headlines. Saved to 'fake_news_india.csv'\n",
      "                                            headline label    source\n",
      "1  Video of Iran Missile Strike On Israel Linked ...  fake  BoomLive\n",
      "0  Video Of Pak Soldiers Viral As Indian Army Wav...  fake  BoomLive\n",
      "2  Doctored Republic Bharat Clip Viral As Pak Arm...  fake  BoomLive\n",
      "7  Murshidabad Riots: BJP WB Uses CAA Protest Pho...  fake  BoomLive\n",
      "8  Video From Maharashtra Viral Claiming Bajrang ...  fake  BoomLive\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "def safe_request(url, delay=1):\n",
    "    try:\n",
    "        response = requests.get(url, timeout=10)\n",
    "        time.sleep(delay)\n",
    "        return response\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Request failed: {e}\")\n",
    "        return None\n",
    "\n",
    "# 1. Alt News\n",
    "def scrape_altnews(pages=3):\n",
    "    headlines = []\n",
    "    for page in range(1, pages + 1):\n",
    "        url = f\"https://www.altnews.in/category/fake-news/page/{page}/\"\n",
    "        res = safe_request(url)\n",
    "        if res:\n",
    "            soup = BeautifulSoup(res.text, \"html.parser\")\n",
    "            for tag in soup.select(\"h3.entry-title a\"):\n",
    "                headlines.append({\n",
    "                    \"headline\": tag.get_text(strip=True),\n",
    "                    \"label\": \"fake\",\n",
    "                    \"source\": \"AltNews\"\n",
    "                })\n",
    "    return headlines\n",
    "\n",
    "# 2. Boom Live\n",
    "def scrape_boomlive(pages=3):\n",
    "    headlines = []\n",
    "    for page in range(1, pages + 1):\n",
    "        url = f\"https://www.boomlive.in/fact-check/{page}\"\n",
    "        res = safe_request(url)\n",
    "        if res:\n",
    "            soup = BeautifulSoup(res.text, \"html.parser\")\n",
    "            for tag in soup.select(\"h4.font-alt.normal.mt-10\"):\n",
    "                headlines.append({\n",
    "                    \"headline\": tag.get_text(strip=True),\n",
    "                    \"label\": \"fake\",\n",
    "                    \"source\": \"BoomLive\"\n",
    "                })\n",
    "    return headlines\n",
    "\n",
    "# 3. Factly\n",
    "def scrape_factly(pages=3):\n",
    "    headlines = []\n",
    "    for page in range(1, pages + 1):\n",
    "        url = f\"https://factly.in/category/fake-news/page/{page}/\"\n",
    "        res = safe_request(url)\n",
    "        if res:\n",
    "            soup = BeautifulSoup(res.text, \"html.parser\")\n",
    "            for tag in soup.select(\"h2.entry-title a\"):\n",
    "                headlines.append({\n",
    "                    \"headline\": tag.get_text(strip=True),\n",
    "                    \"label\": \"fake\",\n",
    "                    \"source\": \"Factly\"\n",
    "                })\n",
    "    return headlines\n",
    "\n",
    "# 🗃️ Aggregate everything\n",
    "def collect_fake_news(total_pages=3):\n",
    "    print(\"Scraping Alt News...\")\n",
    "    altnews = scrape_altnews(pages=total_pages)\n",
    "\n",
    "    print(\"Scraping Boom Live...\")\n",
    "    boomlive = scrape_boomlive(pages=total_pages)\n",
    "\n",
    "    print(\"Scraping Factly...\")\n",
    "    factly = scrape_factly(pages=total_pages)\n",
    "\n",
    "    all_fake = altnews + boomlive + factly\n",
    "    df = pd.DataFrame(all_fake)\n",
    "    df.to_csv(\"fake_news_india.csv\", index=False)\n",
    "    print(f\"✅ Collected {len(df)} fake news headlines. Saved to 'fake_news_india.csv'\")\n",
    "    return df\n",
    "\n",
    "# Run the scraper\n",
    "if __name__ == \"__main__\":\n",
    "    df = collect_fake_news(total_pages=5)\n",
    "    print(df.sample(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f8abb8a5-0566-4185-9434-1cee53a2156c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Python(11305) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Python(11306) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Python(11307) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Python(11308) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Python(11309) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Python(11310) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Python(11312) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Python(11313) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Python(11314) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Python(11315) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Python(11316) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Python(11317) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Python(11318) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔁 Scrolling to load all headlines...\n",
      "✅ Headlines loaded: 3\n",
      "\n",
      "🟢 Total headlines scraped: 3\n",
      "✅ Saved to boomlive_fake_news_fullscroll.csv\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.firefox.service import Service\n",
    "from selenium.webdriver.firefox.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from webdriver_manager.firefox import GeckoDriverManager\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "def scrape_boomlive_full_scroll(url=\"https://www.boomlive.in/fact-check\", max_wait=20):\n",
    "    headlines = []\n",
    "\n",
    "    options = Options()\n",
    "    options.headless = True\n",
    "    driver = webdriver.Firefox(service=Service(GeckoDriverManager().install()), options=options)\n",
    "\n",
    "    try:\n",
    "        driver.get(url)\n",
    "        time.sleep(3)  # initial load\n",
    "\n",
    "        last_count = 0\n",
    "        same_count_times = 0\n",
    "\n",
    "        print(\"🔁 Scrolling to load all headlines...\")\n",
    "\n",
    "        while True:\n",
    "            driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "            time.sleep(2)  # wait for new content\n",
    "\n",
    "            elements = driver.find_elements(By.CSS_SELECTOR, \"h4.font-alt.normal.mt-10\")\n",
    "            curr_count = len(elements)\n",
    "\n",
    "            if curr_count == last_count:\n",
    "                same_count_times += 1\n",
    "            else:\n",
    "                same_count_times = 0\n",
    "\n",
    "            if same_count_times >= 3 or curr_count >= 100 or max_wait <= 0:\n",
    "                break\n",
    "\n",
    "            last_count = curr_count\n",
    "            max_wait -= 1\n",
    "\n",
    "        print(f\"✅ Headlines loaded: {curr_count}\")\n",
    "\n",
    "        for elem in elements:\n",
    "            text = elem.text.strip()\n",
    "            if text:\n",
    "                headlines.append({\n",
    "                    \"headline\": text,\n",
    "                    \"label\": \"fake\",\n",
    "                    \"source\": \"BoomLive\"\n",
    "                })\n",
    "\n",
    "    finally:\n",
    "        driver.quit()\n",
    "\n",
    "    print(f\"\\n🟢 Total headlines scraped: {len(headlines)}\")\n",
    "    return headlines\n",
    "\n",
    "# Run it\n",
    "if __name__ == \"__main__\":\n",
    "    data = scrape_boomlive_full_scroll()\n",
    "    df = pd.DataFrame(data)\n",
    "    df.to_csv(\"boomlive_fake_news_fullscroll.csv\", index=False)\n",
    "    print(\"✅ Saved to boomlive_fake_news_fullscroll.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3ed995ed-c1b3-47f7-b57d-a54425765dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from spellchecker import SpellChecker\n",
    "\n",
    "# Load your dataset\n",
    "df = pd.read_csv(\"Cleaned_FactDrill_Dataset.csv\")\n",
    "\n",
    "# Initialize spell checker\n",
    "spell = SpellChecker(language='en')\n",
    "\n",
    "# Find misspelled words\n",
    "misspelled_records = []\n",
    "for headline in df['headline'].dropna():\n",
    "    words = headline.lower().split()\n",
    "    misspelled = spell.unknown(words)\n",
    "    if misspelled:\n",
    "        misspelled_records.append({\n",
    "            \"headline\": headline,\n",
    "            \"misspelled_words\": list(misspelled)\n",
    "        })\n",
    "\n",
    "# Convert to DataFrame and optionally save\n",
    "misspelled_df = pd.DataFrame(misspelled_records)\n",
    "misspelled_df.to_csv(\"misspelled_headlines.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf10bb91-463e-484d-abd3-f6cd53565a7a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11 (hf_env)",
   "language": "python",
   "name": "hf_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
